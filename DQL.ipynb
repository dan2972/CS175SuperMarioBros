{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext line_profiler\n",
    "# Import the game\n",
    "import gym_super_mario_bros\n",
    "# Import the Joypad wrapper\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "# Import the SIMPLIFIED controls\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "import imageio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from torchvision import transforms as T\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "from pathlib import Path\n",
    "\n",
    "from tensordict import TensorDict\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/cs175/lib/python3.11/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "/opt/miniconda3/envs/cs175/lib/python3.11/site-packages/gym/envs/registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  logger.warn(\n",
      "/opt/miniconda3/envs/cs175/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "env = gym_super_mario_bros.make(\"SuperMarioBros-v0\", render_mode='rgb_array', apply_api_compatibility=True)\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "\n",
    "env.reset()\n",
    "next_state, reward, done, trunc, info = env.step(action=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunk, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape, antialias=True), T.Normalize(0, 255)]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation\n",
    "\n",
    "class CustomRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super(CustomRewardWrapper, self).__init__(env)\n",
    "        self.coin_reward = 5 \n",
    "        self.previous_coins = 0\n",
    "        self.previous_score = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, done, trunc, info = self.env.step(action)\n",
    "\n",
    "        if info['coins'] > self.previous_coins:\n",
    "            reward += self.coin_reward\n",
    "\n",
    "        # Check for score changes\n",
    "        reward += (info['score'] - self.previous_score) / 10.0\n",
    "\n",
    "        # Update previous values\n",
    "        self.previous_coins = info['coins']\n",
    "        self.previous_score = info['score']\n",
    "\n",
    "        return state, reward, done, trunc, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.previous_coins = 0\n",
    "        self.previous_score = 0\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "\n",
    "# Apply Wrappers to environment\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "env = FrameStack(env, num_stack=4)\n",
    "env = CustomRewardWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario:\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Mario's DNN to predict the most optimal action - we implement this in the Learn section\n",
    "        self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
    "        self.net = self.net.to(device=self.device)\n",
    "\n",
    "        self.exploration_rate = 1\n",
    "        self.exploration_rate_decay = 0.99999975\n",
    "        self.exploration_rate_min = 0.1\n",
    "        self.curr_step = 0\n",
    "\n",
    "        self.save_every = 5e5  # no. of experiences between saving Mario Net\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "    Given a state, choose an epsilon-greedy action and update value of step.\n",
    "\n",
    "    Inputs:\n",
    "    state(``LazyFrame``): A single observation of the current state, dimension is (state_dim)\n",
    "    Outputs:\n",
    "    ``action_idx`` (``int``): An integer representing which action Mario will perform\n",
    "    \"\"\"\n",
    "        # EXPLORE\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "\n",
    "        # EXPLOIT\n",
    "        else:\n",
    "            state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
    "            state = torch.tensor(state, device=self.device).unsqueeze(0)\n",
    "            action_values = self.net(state, model=\"online\")\n",
    "            action_idx = torch.argmax(action_values, axis=1).item()\n",
    "\n",
    "        # decrease exploration_rate\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "\n",
    "        # increment step\n",
    "        self.curr_step += 1\n",
    "        return action_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):  # subclassing for continuity\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.memory = TensorDictReplayBuffer(storage=LazyMemmapStorage(100000, device=torch.device(\"cpu\")))\n",
    "        self.batch_size = 32\n",
    "\n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        \"\"\"\n",
    "        Store the experience to self.memory (replay buffer)\n",
    "\n",
    "        Inputs:\n",
    "        state (``LazyFrame``),\n",
    "        next_state (``LazyFrame``),\n",
    "        action (``int``),\n",
    "        reward (``float``),\n",
    "        done(``bool``))\n",
    "        \"\"\"\n",
    "        def first_if_tuple(x):\n",
    "            return x[0] if isinstance(x, tuple) else x\n",
    "        state = first_if_tuple(state).__array__()\n",
    "        next_state = first_if_tuple(next_state).__array__()\n",
    "\n",
    "        state = torch.tensor(state)\n",
    "        next_state = torch.tensor(next_state)\n",
    "        action = torch.tensor([action])\n",
    "        reward = torch.tensor([reward])\n",
    "        done = torch.tensor([done])\n",
    "\n",
    "        # self.memory.append((state, next_state, action, reward, done,))\n",
    "        self.memory.add(TensorDict({\"state\": state, \"next_state\": next_state, \"action\": action, \"reward\": reward, \"done\": done}, batch_size=[]))\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"\n",
    "        Retrieve a batch of experiences from memory\n",
    "        \"\"\"\n",
    "        batch = self.memory.sample(self.batch_size).to(self.device)\n",
    "        state, next_state, action, reward, done = (batch.get(key) for key in (\"state\", \"next_state\", \"action\", \"reward\", \"done\"))\n",
    "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarioNet(nn.Module):\n",
    "    \"\"\"mini CNN structure\n",
    "  input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        c, h, w = input_dim\n",
    "\n",
    "        if h != 84:\n",
    "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
    "        if w != 84:\n",
    "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
    "\n",
    "        self.online = self.__build_cnn(c, output_dim)\n",
    "\n",
    "        self.target = self.__build_cnn(c, output_dim)\n",
    "        self.target.load_state_dict(self.online.state_dict())\n",
    "\n",
    "        # Q_target parameters are frozen.\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == \"online\":\n",
    "            return self.online(input)\n",
    "        elif model == \"target\":\n",
    "            return self.target(input)\n",
    "\n",
    "    def __build_cnn(self, c, output_dim):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.gamma = 0.9\n",
    "\n",
    "    def td_estimate(self, state, action):\n",
    "        current_Q = self.net(state, model=\"online\")[\n",
    "            np.arange(0, self.batch_size), action\n",
    "        ]  # Q_online(s,a)\n",
    "        return current_Q\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def td_target(self, reward, next_state, done):\n",
    "        next_state_Q = self.net(next_state, model=\"online\")\n",
    "        best_action = torch.argmax(next_state_Q, axis=1)\n",
    "        next_Q = self.net(next_state, model=\"target\")[\n",
    "            np.arange(0, self.batch_size), best_action\n",
    "        ]\n",
    "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n",
    "    def update_Q_online(self, td_estimate, td_target):\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def save(self):\n",
    "        save_path = (\n",
    "            self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
    "        )\n",
    "        torch.save(\n",
    "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
    "            save_path,\n",
    "        )\n",
    "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.burnin = 1e4  # min. experiences before training\n",
    "        self.learn_every = 3  # no. of experiences between updates to Q_online\n",
    "        self.sync_every = 1e4  # no. of experiences between Q_target & Q_online sync\n",
    "\n",
    "    def learn(self):\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_target()\n",
    "\n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            self.save()\n",
    "\n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "\n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "\n",
    "        # Sample from memory\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "        # Get TD Estimate\n",
    "        td_est = self.td_estimate(state, action)\n",
    "\n",
    "        # Get TD Target\n",
    "        td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "        # Backpropagate loss through Q_online\n",
    "        loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "        return (td_est.mean().item(), loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class MetricLogger:\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_log = save_dir / \"log\"\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
    "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
    "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
    "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
    "\n",
    "        # History metrics\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        # Moving averages, added for every call to record()\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "        # Current episode metric\n",
    "        self.init_episode()\n",
    "\n",
    "        # Timing\n",
    "        self.record_time = time.time()\n",
    "\n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "\n",
    "    def log_episode(self):\n",
    "        \"Mark end of episode\"\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    def record(self, episode, epsilon, step):\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            f\"Time Delta {time_since_last_record} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        )\n",
    "\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "                f\"{time_since_last_record:15.3f}\"\n",
    "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "            )\n",
    "\n",
    "        for metric in [\"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\", \"ep_rewards\"]:\n",
    "            plt.clf()\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"), label=f\"moving_avg_{metric}\")\n",
    "            plt.legend()\n",
    "            plt.savefig(getattr(self, f\"{metric}_plot\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: False\n",
      "\n",
      "finished episode\n",
      "Episode 0 - Step 288 - Epsilon 0.9999280025829282 - Mean Reward 1069.0 - Mean Length 288.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.461 - Time 2023-12-02T10:57:20\n",
      "finished episode\n",
      "finished episode\n",
      "finished episode\n",
      "finished episode\n",
      "finished episode\n",
      "finished episode\n",
      "finished episode\n",
      "finished episode\n",
      "finished episode\n",
      "finished episode\n",
      "finished episode\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/dan2972/Desktop/UCI/Fall 2023/CS175/CS175SuperMarioBros/DQL.ipynb Cell 12\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dan2972/Desktop/UCI/Fall%202023/CS175/CS175SuperMarioBros/DQL.ipynb#X14sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m mario\u001b[39m.\u001b[39mcache(state, next_state, action, reward, done)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dan2972/Desktop/UCI/Fall%202023/CS175/CS175SuperMarioBros/DQL.ipynb#X14sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# Learn\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/dan2972/Desktop/UCI/Fall%202023/CS175/CS175SuperMarioBros/DQL.ipynb#X14sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m q, loss \u001b[39m=\u001b[39m mario\u001b[39m.\u001b[39mlearn()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dan2972/Desktop/UCI/Fall%202023/CS175/CS175SuperMarioBros/DQL.ipynb#X14sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# Logging\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dan2972/Desktop/UCI/Fall%202023/CS175/CS175SuperMarioBros/DQL.ipynb#X14sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m logger\u001b[39m.\u001b[39mlog_step(reward, loss, q)\n",
      "\u001b[1;32m/Users/dan2972/Desktop/UCI/Fall 2023/CS175/CS175SuperMarioBros/DQL.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dan2972/Desktop/UCI/Fall%202023/CS175/CS175SuperMarioBros/DQL.ipynb#X14sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m td_est \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtd_estimate(state, action)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dan2972/Desktop/UCI/Fall%202023/CS175/CS175SuperMarioBros/DQL.ipynb#X14sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# Get TD Target\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/dan2972/Desktop/UCI/Fall%202023/CS175/CS175SuperMarioBros/DQL.ipynb#X14sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m td_tgt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtd_target(reward, next_state, done)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dan2972/Desktop/UCI/Fall%202023/CS175/CS175SuperMarioBros/DQL.ipynb#X14sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# Backpropagate loss through Q_online\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dan2972/Desktop/UCI/Fall%202023/CS175/CS175SuperMarioBros/DQL.ipynb#X14sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_Q_online(td_est, td_tgt)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/cs175/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;32m/Users/dan2972/Desktop/UCI/Fall 2023/CS175/CS175SuperMarioBros/DQL.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dan2972/Desktop/UCI/Fall%202023/CS175/CS175SuperMarioBros/DQL.ipynb#X14sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m next_state_Q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet(next_state, model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39monline\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dan2972/Desktop/UCI/Fall%202023/CS175/CS175SuperMarioBros/DQL.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m best_action \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(next_state_Q, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/dan2972/Desktop/UCI/Fall%202023/CS175/CS175SuperMarioBros/DQL.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m next_Q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet(next_state, model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m)[\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dan2972/Desktop/UCI/Fall%202023/CS175/CS175SuperMarioBros/DQL.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     np\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size), best_action\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dan2972/Desktop/UCI/Fall%202023/CS175/CS175SuperMarioBros/DQL.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m ]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dan2972/Desktop/UCI/Fall%202023/CS175/CS175SuperMarioBros/DQL.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mreturn\u001b[39;00m (reward \u001b[39m+\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m done\u001b[39m.\u001b[39mfloat()) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m*\u001b[39m next_Q)\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/cs175/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/cs175/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/dan2972/Desktop/UCI/Fall 2023/CS175/CS175SuperMarioBros/DQL.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dan2972/Desktop/UCI/Fall%202023/CS175/CS175SuperMarioBros/DQL.ipynb#X14sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39monline(\u001b[39minput\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dan2972/Desktop/UCI/Fall%202023/CS175/CS175SuperMarioBros/DQL.ipynb#X14sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39melif\u001b[39;00m model \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/dan2972/Desktop/UCI/Fall%202023/CS175/CS175SuperMarioBros/DQL.ipynb#X14sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget(\u001b[39minput\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/cs175/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/cs175/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/cs175/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39m)\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/cs175/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/cs175/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/cs175/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conv_forward(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/cs175/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(\u001b[39minput\u001b[39m, weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"Using CUDA: {use_cuda}\")\n",
    "print()\n",
    "\n",
    "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 40\n",
    "for e in range(episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    # Play the game!\n",
    "    while True:\n",
    "\n",
    "        # Run agent on the state\n",
    "        action = mario.act(state)\n",
    "\n",
    "        # Agent performs action\n",
    "        next_state, reward, done, trunc, info = env.step(action)\n",
    "\n",
    "        # Remember\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # Learn\n",
    "        q, loss = mario.learn()\n",
    "\n",
    "        # Logging\n",
    "        logger.log_step(reward, loss, q)\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if end of game\n",
    "        if done or info[\"flag_get\"]:\n",
    "            print(f\"finished episode {e}\")\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if (e % 20 == 0) or (e == episodes - 1):\n",
    "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs175",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
